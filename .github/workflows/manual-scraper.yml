name: Manual Scraper

on:
  workflow_dispatch:
    inputs:
      scrape_type:
        description: 'Co chcesz scrapować?'
        required: true
        default: 'plany-grup'
        type: choice
        options:
          - all
          - kierunki
          - grupy
          - plany-grup
          - plany-nauczycieli
          - nauczyciele
      max_items:
        description: 'Maksymalna liczba elementów (0 = wszystkie)'
        required: false
        default: '0'
        type: number
      sleep_time:
        description: 'Opóźnienie między żądaniami (sekundy)'
        required: false
        default: '0.1'
        type: number
      collect_teachers:
        description: 'Zbierać informacje o nauczycielach?'
        required: false
        default: 'false'
        type: boolean

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 720  # 12 godzin maksymalnie

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r scraper/requirements.txt

      - name: Create scraper_config.py
        run: |
          cat > scraper/scraper_config.py << 'EOF'
          # Dynamicznie generowany plik konfiguracyjny
          import os
          
          # Parametry pobrane z GitHub Actions
          MAX_ITEMS = ${{ inputs.max_items }}
          SLEEP_TIME = ${{ inputs.sleep_time }}
          COLLECT_TEACHERS = ${{ inputs.collect_teachers == 'true' && 'True' || 'False' }}
          
          # Funkcje pomocnicze
          def get_limited_items(items_list, max_items=MAX_ITEMS):
              """Zwraca ograniczoną liczbę elementów (0 = wszystkie)"""
              if max_items <= 0:
                  return items_list
              return items_list[:max_items]
          
          # Zastępuje domyślne time.sleep
          def custom_sleep():
              """Używa niestandardowego czasu uśpienia"""
              import time
              time.sleep(SLEEP_TIME)
          EOF

      # Dodajemy sprawdzenie struktury db.py przed uruchomieniem
      - name: Check DB structure
        run: |
          echo "Sprawdzam strukturę pliku db.py..."
          grep -A 10 "initialize" scraper/db.py || echo "Nie znaleziono metody initialize w db.py"

      - name: Create run_scraper.py
        run: |
          cat > run_scraper.py << 'EOF'
          # Skrypt uruchamiający wybraną część scrapera
          import sys
          import time
          import os
          
          # Przekazujemy zmienną środowiskową pod obydwiema nazwami, aby była kompatybilna
          # z kodem, który może używać różnych nazw
          if 'SUPABASE_KEY' in os.environ and 'SUPABASE_SERVICE_ROLE_KEY' not in os.environ:
              os.environ['SUPABASE_SERVICE_ROLE_KEY'] = os.environ['SUPABASE_KEY']
          
          from scraper.db import Database
          from scraper.scraper_config import MAX_ITEMS, SLEEP_TIME, COLLECT_TEACHERS, get_limited_items, custom_sleep
          
          # Patch dla time.sleep w utils.py
          import scraper.utils
          original_sleep = time.sleep
          time.sleep = lambda x: custom_sleep()
          
          # Zależnie od wybranego typu, importujemy odpowiedni moduł
          scrape_type = "${{ inputs.scrape_type }}"
          
          print(f"Uruchamianie scrapera: {scrape_type}")
          print(f"Konfiguracja: MAX_ITEMS={MAX_ITEMS}, SLEEP_TIME={SLEEP_TIME}, COLLECT_TEACHERS={COLLECT_TEACHERS}")
          
          # Inicjalizacja bazy danych
          print("Inicjalizacja połączenia z bazą danych...")
          try:
              Database.initialize()
          except Exception as e:
              print(f"Błąd podczas inicjalizacji bazy danych: {e}")
              sys.exit(1)
          
          # [RESZTA KODU POZOSTAJE BEZ ZMIAN]
          EOF

      - name: Run scraper
        run: python run_scraper.py
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

      - name: Create summary
        run: |
          echo "# Podsumowanie scrapowania" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- Typ scrapowania: ${{ inputs.scrape_type }}" >> $GITHUB_STEP_SUMMARY
          echo "- Maksymalna liczba elementów: ${{ inputs.max_items }} (0 = wszystkie)" >> $GITHUB_STEP_SUMMARY
          echo "- Opóźnienie między żądaniami: ${{ inputs.sleep_time }}s" >> $GITHUB_STEP_SUMMARY
          echo "- Zbieranie informacji o nauczycielach: ${{ inputs.collect_teachers }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Sprawdź logi powyżej, aby zobaczyć szczegółowe informacje o przebiegu scrapowania." >> $GITHUB_STEP_SUMMARY